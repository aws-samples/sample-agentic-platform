{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Building Autonomous Agents: Exploring Agent Frameworks:\n",
    "\n",
    "In this module, we'll examine how different agent frameworks implement autonomous agents, focusing specifically on LangChain/LangGraph, PydanticAI, CrewAI, and Strands. We'll explore how these frameworks handle orchestration, tool use, and agent coordination while leveraging our existing abstractions.\n",
    "\n",
    "Objectives:\n",
    "* Get hands on with high-level frameworks like LangChain/LangGraph, PydanticAI, CrewAI, and Strands\n",
    "* Learn how to integrate our tool calling, memory, and conversation abstractions with each framework\n",
    "* Implement examples showing how to maintain consistent interfaces across frameworks\n",
    "* Understand when to use each framework based on their strengths and application needs\n",
    "\n",
    "By the end of this module, you'll understand how to build on top of these frameworks while reusing your existing code, allowing you to choose the right framework for each use case without starting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "We've already explored LangGraph a bit in module 2, but we haven't spent much time with LangChain. In this section, we'll be using the langchain-aws repo to play with LangChain on Bedrock. We'll try to accomplish 3 things\n",
    "* Discuss abstraction layers\n",
    "* Discuss pros/cons\n",
    "* Recreate the agent we built in the previous notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets start with a very simple chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Bedrock chat model\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.messages import AIMessage, BaseMessage\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model=\"us.anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Invoke the llm\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"Hello! How are you today?\"),\n",
    "]\n",
    "\n",
    "response: AIMessage =llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting LangChain to work out of the box is very simple. Now lets recreate the agent we built in the previous lab with LangChain.\n",
    "\n",
    "First lets reuse our tools from the previous lab. We can leverage a nice abstraction LangGraph provides called create_react_agent() to take these tools and quickly create an agent. \n",
    "\n",
    "In essence, this abstracts away a lot of the work we did in the previous notebook to build a ReACT like agent! One cool thing about frameworks is that they usually take in \"callable's\" meaning you can just pass in a function with a doc string and it'll work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List, Callable\n",
    "\n",
    "# Import our tools from the previous lab.\n",
    "from agentic_platform.core.tool.sample_tools import weather_report, handle_calculation\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.graph import Graph\n",
    "\n",
    "# Use the prebuilt react agent from LangGraph\n",
    "agent: Graph = create_react_agent(model=llm, tools=[weather_report, handle_calculation])\n",
    "\n",
    "# Invoke the agent\n",
    "inputs = {\"messages\": [(\"user\", \"What's the weather in San Francisco?\")]}\n",
    "response = agent.invoke(inputs)\n",
    "# Print the response\n",
    "for message in response['messages']:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! In just a couple lines of code were able to recreate the work we did in a previous 3 labs!\n",
    "\n",
    "## A Word on Lock-in. \n",
    "Using LangGraph / LangChain (or any framework) makes sense in scenarios like this. It does exactly what we need it to do out of the box. However, it has it's own types (messages), it's own model invocation implementation, etc.. You can use them, but it creates a 1-way door decision. If you need to build something custom, use different framework, swap out a long term memory implementation, etc.. it will be very painful to undo the tight coupling of a framework.\n",
    "\n",
    "To solve this, we just need to wrap the code above into our own types and then the rest of the system doesn't care what framework you're using. It's a little extra work but provides a lot more flexibility. As things become more standard, this might become less of a problem. But for now, the best way to create 2-way doors is to use your own types. \n",
    "\n",
    "Now lets wrap the agent above in our own abstractions to create interoperability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is undifferentiated converter code so we pushed it to the common folder.\n",
    "# LangChains message format differs significantly from other model APIs so we had to take some short cuts.\n",
    "# Ex) Tool Calls are converted into strings even though it's a pydantic model. \n",
    "# LangChain also has the concept of a tool message which some providers use but others don't.\n",
    "# This is why we have a converter.\n",
    "from agentic_platform.core.converter.langchain_converters import LangChainMessageConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets build our agent and use our abstractions to create interoperability between our custom agent and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.core.models.api_models import AgenticRequest, AgenticResponse\n",
    "from agentic_platform.core.models.prompt_models import BasePrompt\n",
    "from agentic_platform.core.models.memory_models import Message, SessionContext\n",
    "from typing import Dict, Any, List, Callable\n",
    "\n",
    "# Lets reuse our memory client from the previous lab.\n",
    "# Clients.\n",
    "class MemoryClient:\n",
    "    \"\"\"Manages conversations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.conversations: Dict[str, SessionContext] = {}\n",
    "\n",
    "    def upsert_conversation(self, conversation: SessionContext) -> bool:\n",
    "        self.conversations[conversation.session_id] = conversation\n",
    "\n",
    "    def get_or_create_conversation(self, conversation_id: str=None) -> SessionContext:\n",
    "        return self.conversations.get(conversation_id, SessionContext()) if conversation_id else SessionContext()\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.graph import Graph\n",
    "\n",
    "memory_client: MemoryClient = MemoryClient()\n",
    "\n",
    "class LangChainAgent:\n",
    "    \n",
    "    def __init__(self, tools: List[Callable], base_prompt: BasePrompt):\n",
    "        # Do some conversions to take our types and make them work with LangChain.\n",
    "        temp: float = base_prompt.hyperparams[\"temperature\"] if \"temperature\" in base_prompt.hyperparams else 0.5\n",
    "        llm: ChatBedrockConverse = ChatBedrockConverse(\n",
    "            model=base_prompt.model_id,\n",
    "            temperature=temp\n",
    "        )\n",
    "\n",
    "        # We'll use a prebuilt graph from langgraph that implements the same React pattern.\n",
    "        # This should be done at instantiation time to reduce the overhead of re-compiling the graph.\n",
    "        self.agent: Graph = create_react_agent(model=llm, tools=tools)\n",
    "        self.conversation: SessionContext = None\n",
    "\n",
    "    def invoke(self, request: AgenticRequest) -> AgenticResponse:\n",
    "        # Get or create conversation\n",
    "        self.conversation = memory_client.get_or_create_conversation(request.session_id)\n",
    "        # Add user message to conversation\n",
    "        self.conversation.add_message(request.message)\n",
    "        # Convert to langchain messages\n",
    "        inputs = {\"messages\": [(\"user\", request.message.text)]}\n",
    "        response = self.agent.invoke(inputs)\n",
    "        print(response['messages'])\n",
    "        # Convert to our response format\n",
    "        messages: List[Message] = LangChainMessageConverter.convert_langchain_messages(response['messages'])\n",
    "        # Add messages to conversation\n",
    "        self.conversation.add_messages(messages)\n",
    "        # Save the conversation\n",
    "        memory_client.upsert_conversation(self.conversation)\n",
    "        # Return the response\n",
    "        return AgenticResponse(\n",
    "            session_id=self.conversation.session_id,\n",
    "            message=self.conversation.messages[-1]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.core.tool.sample_tools import weather_report, handle_calculation\n",
    "\n",
    "# Define our agent prompt.\n",
    "class AgentPrompt(BasePrompt):\n",
    "    system_prompt: str = '''You are a helpful assistant.'''\n",
    "    user_prompt: str = '''{user_message}'''\n",
    "\n",
    "# Build out our prompt\n",
    "user_message: str = 'What is the weather in San Francisco?'\n",
    "prompt: BasePrompt = AgentPrompt()\n",
    "# Instantiate the agent\n",
    "\n",
    "tools: List[Callable] = [weather_report, handle_calculation]\n",
    "my_agent: LangChainAgent = LangChainAgent(base_prompt=prompt, tools=tools) \n",
    "\n",
    "# Create the agent request. Same as our other agent type in the tool calling lab.\n",
    "request: AgenticRequest = AgenticRequest.from_text(text=user_message)\n",
    "\n",
    "# Invoke the agent\n",
    "response: AgenticResponse = my_agent.invoke(request)\n",
    "\n",
    "print(response.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets look at our conversation.\n",
    "conversation: SessionContext = memory_client.get_or_create_conversation(response.session_id)\n",
    "# Use the pydantic model_dump_json\n",
    "print(conversation.model_dump_json(indent=2, serialize_as_any=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have interoperability!\n",
    "By adding some converters and wrapping LangChain/LangGraph in our own abstractions, we were able to \n",
    "1. Return the same agent response as our custom agent so we can reuse AgentRequest & AgentResponse types\n",
    "2. Have a universal memory implementation across different agents built with different frameworks\n",
    "3. Decoupled ourselves from relying too much on any framework, API provider, etc.. \n",
    "\n",
    "By owning our own types, we can begin to see how you can mix and match frameworks to get the best of both worlds while still maintaining control over your system. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic AI\n",
    "Pydantic AI is a newer framework. The main draw is type safety. you can create type safe graphs and build agents relatively quickly. It also works really well with pydantic models. \n",
    "\n",
    "In the examples with LangChain above, the react agent can't handle the nested pydantic objects of the calculator function and will usually error out unless you change your function definition. PydanticAI doesn't have that problem.\n",
    "\n",
    "It is fairly new (as of 4/20/2025) but is a framework worth watching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to use nest_asyncio which patches the asyncio to allow nested event loops which PydanticAI runs on.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Lets create a simple agent. We need to start aliasing the agent class to avoid conflict with the langchain agent.\n",
    "from pydantic_ai import Agent as PyAIAgent\n",
    "\n",
    "pyai_agent: PyAIAgent = PyAIAgent(\n",
    "    'bedrock:anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    system_prompt='You are a helpful assistant.',\n",
    ")\n",
    "\n",
    "# Now lets add our existing tools to the agent. Notice how the tool object actually lives on the agent object itself. \n",
    "# Secondly, PydanticAI has two types of tools. tool() has access to the run context while tool_plain() does not.\n",
    "# We'll use the plain tool here since we don't need access to the run context.\n",
    "tools: List[Tool] = [pyai_agent.tool_plain(func)for func in [weather_report, handle_calculation]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sync = pyai_agent.run_sync('What is 7 plus 7?')\n",
    "print(result_sync.data)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "for message in result_sync.all_messages():\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create our Abstraction Layers\n",
    "Pydantic AI has a pretty clean design around agents. However, we still want to own our own types to make it interoperable with other parts of our system. Lets create our wrappers and converters like with did with langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like the langchain converter, this is undifferentiated code so we pushed it to the common folder.\n",
    "from agentic_platform.core.converter.pydanticai_converters import PydanticAIMessageConverter\n",
    "\n",
    "messages: List[Message] = PydanticAIMessageConverter.convert_messages(result_sync.all_messages())\n",
    "\n",
    "for message in messages:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewrite Our Agent in Pydantic\n",
    "Now we can rewrite our agent in Pydantic. We'll reuse the same memory client to show how we can store conversations across various frameworks / models without having to rewrite our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent as PyAIAgent\n",
    "from pydantic_ai.models import ModelResponse\n",
    "\n",
    "class PydanticAIAgent:\n",
    "    \n",
    "    def __init__(self, tools: List[Callable], base_prompt: BasePrompt):\n",
    "        # This is the identifier for PydanticAI calling Bedrock.\n",
    "        model_id = f'bedrock:{base_prompt.model_id}'\n",
    "        self.agent: PyAIAgent = PyAIAgent(\n",
    "            model_id,\n",
    "            system_prompt=base_prompt.system_prompt,\n",
    "        )\n",
    "\n",
    "        # Add our tools to the agent.\n",
    "        [self.agent.tool_plain(func)for func in tools]\n",
    "\n",
    "    def invoke(self, request: AgenticRequest) -> AgenticResponse:\n",
    "        # Get or create conversation\n",
    "        conversation: SessionContext = memory_client.get_or_create_conversation(request.session_id)\n",
    "        # Convert to langchain messages\n",
    "        response: ModelResponse = self.agent.run_sync(request.message.text)\n",
    "        # Convert to our response format\n",
    "        messages: List[Message] = PydanticAIMessageConverter.convert_messages(response.all_messages())\n",
    "        # Add messages to conversation\n",
    "        conversation.add_messages(messages)\n",
    "        # Save the conversation\n",
    "        memory_client.upsert_conversation(conversation)\n",
    "        # Return the response\n",
    "        return AgenticResponse(\n",
    "            session_id=conversation.session_id,\n",
    "            message=messages[-1]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our agent prompt.\n",
    "class AgentPrompt(BasePrompt):\n",
    "    system_prompt: str = '''You are a helpful assistant.'''\n",
    "    user_prompt: str = '''{user_message}'''\n",
    "\n",
    "# Build out our prompt\n",
    "user_message: str = 'What is the weather in San Francisco?'\n",
    "prompt: BasePrompt = AgentPrompt()\n",
    "# Instantiate the agent\n",
    "my_agent: PydanticAIAgent = PydanticAIAgent(base_prompt=prompt, tools=tools) \n",
    "\n",
    "# Create the agent request. Same as our other agent type in the tool calling lab.\n",
    "request: AgenticRequest = AgenticRequest.from_text(text=user_message)\n",
    "\n",
    "# Invoke the agent\n",
    "response: AgenticResponse = my_agent.invoke(request)\n",
    "\n",
    "print(response.message.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets look at our conversation.\n",
    "conversation: SessionContext = memory_client.get_or_create_conversation(response.session_id)\n",
    "# Use the pydantic model_dump_json\n",
    "print(conversation.model_dump_json(indent=2, serialize_as_any=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strands\n",
    "Strands is a modern agent framework that provides a clean, simple API for building agents with native LiteLLM integration. It's designed to be lightweight and easy to use while still providing powerful agent capabilities.\n",
    "\n",
    "Key features of Strands:\n",
    "* Native LiteLLM integration for model flexibility\n",
    "* Simple, intuitive API\n",
    "* Built-in tool ecosystem via strands-tools\n",
    "* Lightweight and performant\n",
    "\n",
    "Let's explore how to use Strands with our existing abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a simple Strands agent\n",
    "from strands import Agent as StrandsAgent\n",
    "from strands.models.litellm import LiteLLMModel as StrandsLiteLLMModel\n",
    "from strands_tools import calculator\n",
    "\n",
    "# Create a LiteLLM model for Strands\n",
    "model = StrandsLiteLLMModel(\n",
    "    model_id=\"bedrock/us.anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    params={\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create a simple agent with built-in calculator tool\n",
    "agent = StrandsAgent(model=model, tools=[calculator])\n",
    "\n",
    "# Test the agent\n",
    "response = agent(\"What is 15 * 23?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's integrate our custom tools with Strands. Strands can work with regular Python functions, making it easy to integrate our existing tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom tools\n",
    "from agentic_platform.core.tool.sample_tools import weather_report, handle_calculation\n",
    "\n",
    "# Create agent with our custom tools\n",
    "strands_agent = StrandsAgent(model=model, tools=[weather_report, handle_calculation])\n",
    "\n",
    "# Test with weather query\n",
    "response = strands_agent(\"What's the weather like in New York?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create our Strands Abstraction Layer\n",
    "Like with the other frameworks, we want to wrap Strands in our own abstractions to maintain interoperability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple wrapper for Strands that integrates with our memory system\n",
    "from strands import Agent as StrandsAgent\n",
    "from strands.models.litellm import LiteLLMModel as StrandsLiteLLMModel\n",
    "\n",
    "class StrandsAgentWrapper:\n",
    "    \n",
    "    def __init__(self, tools: List[Callable], base_prompt: BasePrompt):\n",
    "        # Create LiteLLM model with our prompt configuration\n",
    "        temp: float = base_prompt.hyperparams.get(\"temperature\", 0.5)\n",
    "        max_tokens: int = base_prompt.hyperparams.get(\"max_tokens\", 1000)\n",
    "        \n",
    "        self.model = StrandsLiteLLMModel(\n",
    "            model_id=f\"bedrock/{base_prompt.model_id}\",\n",
    "            params={\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temp,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Create the Strands agent\n",
    "        self.agent = StrandsAgent(\n",
    "            model=self.model, \n",
    "            tools=tools,\n",
    "            system_prompt=base_prompt.system_prompt\n",
    "        )\n",
    "        \n",
    "    def invoke(self, request: AgenticRequest) -> AgenticResponse:\n",
    "        # Get or create conversation\n",
    "        conversation: SessionContext = memory_client.get_or_create_conversation(request.session_id)\n",
    "        \n",
    "        # Add user message to conversation\n",
    "        conversation.add_message(request.message)\n",
    "        \n",
    "        # Extract text from the message for Strands\n",
    "        user_text = \"\"\n",
    "        if request.message.content:\n",
    "            for content in request.message.content:\n",
    "                if hasattr(content, 'text') and content.text:\n",
    "                    user_text = content.text\n",
    "                    break\n",
    "        \n",
    "        # Call Strands agent\n",
    "        response_text = self.agent(user_text)\n",
    "        \n",
    "        # Create response message\n",
    "        response_message = Message(\n",
    "            role=\"assistant\",\n",
    "            content=[TextContent(type=\"text\", text=response_text)]\n",
    "        )\n",
    "        \n",
    "        # Add to conversation\n",
    "        conversation.add_message(response_message)\n",
    "        \n",
    "        # Save conversation\n",
    "        memory_client.upsert_conversation(conversation)\n",
    "        \n",
    "        # Return response\n",
    "        return AgenticResponse(\n",
    "            session_id=conversation.session_id,\n",
    "            message=response_message\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our wrapped Strands agent\n",
    "from agentic_platform.core.tool.sample_tools import weather_report, handle_calculation\n",
    "\n",
    "# Define our agent prompt\n",
    "class StrandsAgentPrompt(BasePrompt):\n",
    "    system_prompt: str = '''You are a helpful assistant.'''\n",
    "    user_prompt: str = '''{user_message}'''\n",
    "\n",
    "# Build our prompt\n",
    "user_message: str = 'What is the weather in Seattle?'\n",
    "prompt: BasePrompt = StrandsAgentPrompt()\n",
    "\n",
    "# Instantiate the agent\n",
    "tools: List[Callable] = [weather_report, handle_calculation]\n",
    "my_strands_agent: StrandsAgentWrapper = StrandsAgentWrapper(base_prompt=prompt, tools=tools)\n",
    "\n",
    "# Create the agent request\n",
    "request: AgenticRequest = AgenticRequest.from_text(text=user_message)\n",
    "\n",
    "# Invoke the agent\n",
    "response: AgenticResponse = my_strands_agent.invoke(request)\n",
    "\n",
    "print(response.message.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our conversation\n",
    "conversation: SessionContext = memory_client.get_or_create_conversation(response.session_id)\n",
    "print(conversation.model_dump_json(indent=2, serialize_as_any=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This concludes module 3 on autonomous agents. In this lab we:\n",
    "1. Explored 3 of the many agent frameworks available today: LangChain/LangGraph, PydanticAI, and Strands\n",
    "2. Demonstrated how to make agent frameworks interoperable and create 2 way door decisions with proper abstraction in code\n",
    "3. Showed how different frameworks have different strengths - LangGraph for complex workflows, PydanticAI for type safety, and Strands for simplicity\n",
    "\n",
    "In the next module we'll be discussing some more advanced concepts of agents. Specifically multi-agent systems and model context protocol (MCP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-program-technical-assets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
